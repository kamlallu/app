# -*- coding: utf-8 -*-
"""Rag with LLm (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eS_BQhEYV-13XU8VNyU2rtbI2AxfPUay
"""

pip install transformers faiss-cpu sentence-transformers PyPDF2

pip install PyMuPDF

pip install sentencepiece

pip install -U langchain-community

from transformers import T5ForConditionalGeneration, T5Tokenizer
import fitz

model_name = "google/flan-t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)
text = ""

import fitz  # PyMuPDF

def extract_text_from_pdf(pdf_file_path):
    text = ""
    with fitz.open(pdf_file_path) as pdf_document:
        num_pages = pdf_document.page_count
        for page_num in range(num_pages):
            page = pdf_document[page_num]
            text += page.get_text()
    return text

# Path to the PDF file
pdf_file_path = "C:/Users/priya_0yiyr9i/Downloads/Meta-03-31-2024-Exhibit-99-1_FINAL.pdf"

# Extract text from the PDF
pdf_text = extract_text_from_pdf(pdf_file_path)

from transformers import T5ForConditionalGeneration, T5Tokenizer

# Initialize T5 model and tokenizer
model_name = "google/flan-t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# Dummy function to retrieve relevant passages (replace with actual retrieval logic)
def retrieve_relevant_passages(question):
    # Placeholder logic (replace with actual retrieval logic)
    return "Meta Platforms, Inc. is a global technology company that develops platforms for people to connect."

# Define multiple questions
questions = [
    "What is the main topic of the document?",
    "Who are the key stakeholders mentioned?",
    "What are the financial highlights discussed?",
    "What is the conclusion of the report?",
]

# Iterate through questions and generate answers using RAG
for idx, question in enumerate(questions, start=1):
    print(f"Question {idx}: {question}")

    # Retrieve relevant passages
    retrieved_passages = retrieve_relevant_passages(question)

    # Generate answer using RAG approach
    inputs = tokenizer.encode("question: " + question + " context: " + retrieved_passages, return_tensors="pt", max_length=512, truncation=True)

    # Adjust max_length for longer answers
    outputs = model.generate(inputs, max_length=1000)  # Increase max_length as needed

    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print(f"Answer {idx}: {answer}")
    print()



















